{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "N_HIDDEN = 128\n",
    "OBSERVE_LENGTH = 10\n",
    "PREDICT_LENGTH = 5\n",
    "FEAT_DIM = 12\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "IMG_DIM = 2048\n",
    "VAL_RATIO = 0.1\n",
    "TRAIN_FOLDERS = '/home/dataset/training_observe_{}_predict_{}/*/'.format(OBSERVE_LENGTH, PREDICT_LENGTH)\n",
    "# TEST_FOLDERS = '/home/dataset/validation_observe_{}_predict_{}/*/'.format(OBSERVE_LENGTH, PREDICT_LENGTH)\n",
    "\n",
    "MODEL_NAME = 'sslstm-image-front_epoch_{}_hidden_{}_observe_{}_predict_{}'.format(EPOCHS, N_HIDDEN, OBSERVE_LENGTH, PREDICT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train folder num: 18000\n",
      "val folder num: 2000\n"
     ]
    }
   ],
   "source": [
    "train_folders = glob.glob(TRAIN_FOLDERS)\n",
    "random.shuffle(train_folders)\n",
    "val_num = int(VAL_RATIO * len(train_folders))\n",
    "val_folders = train_folders[-val_num:]\n",
    "train_folders = train_folders[:-val_num]\n",
    "\n",
    "print('train folder num:', len(train_folders))\n",
    "print('val folder num:', len(val_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X_image = np.empty((self.batch_size, OBSERVE_LENGTH, IMG_DIM))\n",
    "        X_feat = np.empty((self.batch_size, OBSERVE_LENGTH, FEAT_DIM))\n",
    "        y = np.empty((self.batch_size, PREDICT_LENGTH, 3), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            image_seq = []\n",
    "            with open(ID + 'img_path_0.txt', 'r') as f:\n",
    "                img_path = f.read().split('\\n')\n",
    "\n",
    "            for p in img_path:\n",
    "                image_seq.append(np.load(p))\n",
    "            \n",
    "            X_image[i,] = np.array(image_seq)\n",
    "            X_feat[i,] = np.load(ID + 'X.npy')[:, :-3]\n",
    "            y[i] = np.load(ID + 'y.npy')\n",
    "\n",
    "        return [X_image, X_feat], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    opt = optimizers.RMSprop(lr=LEARNING_RATE)\n",
    "    \n",
    "    \n",
    "    feat_input = Input(shape=(OBSERVE_LENGTH, FEAT_DIM))\n",
    "    img_input = Input(shape=(OBSERVE_LENGTH, IMG_DIM))\n",
    "    \n",
    "    #encoder_feat\n",
    "    encoder_feat = layers.GRU(N_HIDDEN,\n",
    "                  input_shape=(OBSERVE_LENGTH, FEAT_DIM),\n",
    "                  return_sequences=False,\n",
    "                  stateful=False,\n",
    "                  dropout=0.2)(feat_input)\n",
    "\n",
    "    encoder_img = layers.GRU(N_HIDDEN,\n",
    "                  input_shape=(OBSERVE_LENGTH, IMG_DIM),\n",
    "                  return_sequences=False,\n",
    "                  stateful=False,\n",
    "                  dropout=0.2)(img_input)\n",
    "        \n",
    "    concated = layers.concatenate([encoder_img, encoder_feat])\n",
    "    \n",
    "    rv = layers.RepeatVector(PREDICT_LENGTH)(concated)\n",
    "    \n",
    "    #lstm decoder\n",
    "    decoder = layers.GRU(N_HIDDEN,\n",
    "                  return_sequences=True,\n",
    "                  stateful=False,\n",
    "                  dropout=0.2)(rv)\n",
    "    dense = layers.TimeDistributed(layers.Dense(3), input_shape=(PREDICT_LENGTH, None))(decoder)\n",
    "    out = layers.Activation('linear')(dense)\n",
    "    \n",
    "    model = Model(inputs=[img_input, feat_input], outputs=[out])\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zg2309/anaconda3/envs/waymo/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 10, 2048)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 10, 12)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 128)          835968      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, 128)          54144       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           gru_1[0][0]                      \n",
      "                                                                 gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 5, 256)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 5, 128)       147840      repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 5, 3)         387         gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 5, 3)         0           time_distributed[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 1,038,339\n",
      "Trainable params: 1,038,339\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From /home/zg2309/anaconda3/envs/waymo/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.2727Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 57s - loss: 0.2031\n",
      "Epoch 00001: val_loss improved from inf to 0.20310, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 84s 298ms/step - loss: 0.2724 - val_loss: 0.2031\n",
      "Epoch 2/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.2231Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 46s - loss: 0.1982\n",
      "Epoch 00002: val_loss improved from 0.20310 to 0.19825, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 62s 221ms/step - loss: 0.2228 - val_loss: 0.1982\n",
      "Epoch 3/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.2134\n",
      "Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 1:00 - loss: 0.1970\n",
      "Epoch 00003: val_loss improved from 0.19825 to 0.19704, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 75s 268ms/step - loss: 0.2135 - val_loss: 0.1970\n",
      "Epoch 4/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.2077Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 1:17 - loss: 0.1925\n",
      "Epoch 00004: val_loss improved from 0.19704 to 0.19245, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 79s 283ms/step - loss: 0.2078 - val_loss: 0.1925\n",
      "Epoch 5/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.2032Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 34s - loss: 0.1899\n",
      "Epoch 00005: val_loss improved from 0.19245 to 0.18989, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 63s 223ms/step - loss: 0.2031 - val_loss: 0.1899\n",
      "Epoch 6/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.2002Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 29s - loss: 0.1840\n",
      "Epoch 00006: val_loss improved from 0.18989 to 0.18404, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 49s 175ms/step - loss: 0.2001 - val_loss: 0.1840\n",
      "Epoch 7/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1963Epoch 1/1000\n",
      " 30/281 [==>...........................] - ETA: 30s - loss: 0.1790\n",
      "Epoch 00007: val_loss improved from 0.18404 to 0.18012, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 159ms/step - loss: 0.1966 - val_loss: 0.1801\n",
      "Epoch 8/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1935Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 34s - loss: 0.1754\n",
      "Epoch 00008: val_loss improved from 0.18012 to 0.17544, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 161ms/step - loss: 0.1934 - val_loss: 0.1754\n",
      "Epoch 9/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1902Epoch 1/1000\n",
      "Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 33s - loss: 0.1743\n",
      "Epoch 00009: val_loss improved from 0.17544 to 0.17426, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 48s 171ms/step - loss: 0.1903 - val_loss: 0.1743\n",
      "Epoch 10/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1879Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 34s - loss: 0.1694\n",
      "Epoch 00010: val_loss improved from 0.17426 to 0.16939, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 52s 183ms/step - loss: 0.1878 - val_loss: 0.1694\n",
      "Epoch 11/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1852Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 29s - loss: 0.1669\n",
      "Epoch 00011: val_loss improved from 0.16939 to 0.16692, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 47s 166ms/step - loss: 0.1854 - val_loss: 0.1669\n",
      "Epoch 12/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1832Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 32s - loss: 0.1657\n",
      "Epoch 00012: val_loss improved from 0.16692 to 0.16574, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 161ms/step - loss: 0.1831 - val_loss: 0.1657\n",
      "Epoch 13/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1799Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 29s - loss: 0.1620\n",
      "Epoch 00013: val_loss improved from 0.16574 to 0.16195, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 46s 162ms/step - loss: 0.1802 - val_loss: 0.1620\n",
      "Epoch 14/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1792Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 29s - loss: 0.1609\n",
      "Epoch 00014: val_loss improved from 0.16195 to 0.16089, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 160ms/step - loss: 0.1792 - val_loss: 0.1609\n",
      "Epoch 15/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1773Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 32s - loss: 0.1599\n",
      "Epoch 00015: val_loss improved from 0.16089 to 0.15993, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 159ms/step - loss: 0.1773 - val_loss: 0.1599\n",
      "Epoch 16/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1751Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 30s - loss: 0.1620\n",
      "Epoch 00016: val_loss did not improve from 0.15993\n",
      "281/281 [==============================] - 46s 165ms/step - loss: 0.1752 - val_loss: 0.1620\n",
      "Epoch 17/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1744Epoch 1/1000\n",
      " 30/281 [==>...........................] - ETA: 29s - loss: 0.1566\n",
      "Epoch 00017: val_loss improved from 0.15993 to 0.15601, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 159ms/step - loss: 0.1742 - val_loss: 0.1560\n",
      "Epoch 18/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1736Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 31s - loss: 0.1556\n",
      "Epoch 00018: val_loss improved from 0.15601 to 0.15561, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 160ms/step - loss: 0.1737 - val_loss: 0.1556\n",
      "Epoch 19/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1720Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 28s - loss: 0.1589\n",
      "Epoch 00019: val_loss did not improve from 0.15561\n",
      "281/281 [==============================] - 45s 161ms/step - loss: 0.1722 - val_loss: 0.1589\n",
      "Epoch 20/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1717Epoch 1/1000\n",
      " 30/281 [==>...........................] - ETA: 30s - loss: 0.1543\n",
      "Epoch 00020: val_loss improved from 0.15561 to 0.15487, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 160ms/step - loss: 0.1718 - val_loss: 0.1549\n",
      "Epoch 21/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1696Epoch 1/1000\n",
      " 30/281 [==>...........................] - ETA: 31s - loss: 0.1537\n",
      " 31/281 [==>...........................] - ETA: 31s - loss: 0.1545\n",
      "Epoch 00021: val_loss improved from 0.15487 to 0.15447, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 160ms/step - loss: 0.1698 - val_loss: 0.1545\n",
      "Epoch 22/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1691Epoch 1/1000\n",
      " 30/281 [==>...........................] - ETA: 29s - loss: 0.1532\n",
      "Epoch 00022: val_loss improved from 0.15447 to 0.15322, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 45s 159ms/step - loss: 0.1691 - val_loss: 0.1532\n",
      "Epoch 23/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1677Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 35s - loss: 0.1523\n",
      "Epoch 00023: val_loss improved from 0.15322 to 0.15232, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 50s 178ms/step - loss: 0.1676 - val_loss: 0.1523\n",
      "Epoch 24/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1674Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 35s - loss: 0.1512\n",
      "Epoch 00024: val_loss improved from 0.15232 to 0.15119, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 0.1674 - val_loss: 0.1512\n",
      "Epoch 25/1000\n",
      "280/281 [============================>.] - ETA: 0s - loss: 0.1669Epoch 1/1000\n",
      " 31/281 [==>...........................] - ETA: 33s - loss: 0.1493\n",
      "Epoch 00025: val_loss improved from 0.15119 to 0.14930, saving model to /home/zg2309/model/sslstm-image-front_epoch_1000_hidden_128_observe_10_predict_5.h5\n",
      "281/281 [==============================] - 51s 182ms/step - loss: 0.1671 - val_loss: 0.1493\n",
      "Epoch 26/1000\n",
      " 48/281 [====>.........................] - ETA: 38s - loss: 0.1697"
     ]
    }
   ],
   "source": [
    "# Aggregated Training Error\n",
    "model = build_model()\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_folders)\n",
    "val_generator = DataGenerator(val_folders)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"/home/zg2309/model/{}.h5\".format(MODEL_NAME), verbose=1, save_best_only=True)\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', mode='auto')\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    workers=6,\n",
    "                    callbacks=[checkpointer])\n",
    "# history = model.fit(train_X, train_y, validation_split=val_ratio, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "fig = plt.gcf()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('/home/zg2309/history/{}'.format(MODEL_NAME))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
